# Заметки и выводы, которые я понемногу делаю, изучая разные подходы в ML

- https://habr.com/ru/companies/slurm/articles/768806/ 

### Тут будет разделение на ML classic --- MLM/LLM --- AutoML и все, что с ними связано
```
потом опишу тут полноценное дерево, пока буду писать только конкретные выводы 
```

### Работа через pipeline transformers (few-shot -- zero-shot learning)

-  pipeline подразумевает верхоуровневое решение, надстройку над токенизацией, векторизацией и части предобработкой;

- не адаптирован под все решения на рынке ML;

- редко подходит под реальный продакшен;

- низкая точность при zero-shot learning (<0.1).

### Токенизация 

- https://habr.com/ru/articles/854664/ 

- токенизация представляет собой процесс разделения входных элементов, которыми могут быть к примеру запросы пользователя, элементы словаря, в целом любые предложения и разделения их на отдельные элементы, дробление их, чтобы их потом было прощее аппроксимировать (преобразовывать для упрощение чтения этих данных машиной);

- токенизаторы бывают разные( BERT и GPT токенизаторы отличаются), и имеют в себе заранее заложенный словарь слов и символов, которые и позволяет разделять входные данные на токены;

- конечный вид токенизированного объекта зависит от типа токенизации и словаря самого токенизатора;

- в рабочей программе импортируются определенные токенизаторы под определенную задачу (задачи классификации намерений и классификация вопрос-ответ); 

- к примему если слова на входе нет словаре токенизатора, он может разделить его некоректно, чтобы окажет негативное влияние на результат аппроксимации;  

- НЕ РЕКОМЕНДУЕТСЯ ТОКЕНИЗИРОВАТЬ ИЛИ СКАРМЛИВАТЬ МОДЕЛЯМ:
```
- сущности (фамилии, номера, аббревиатуры, адреса);
- справочные материалы (расписание работы, телефоны, цены, размеры);
- простые паттерны (приветствие, прощание, типизированные для рабочего случая короткие idшники);
- {
    'КТ':'плохой пример';
    'КТ - это часто используемая аабревиатрура для контрольной точки':'хороший пример'
}
```
- для них лучше подойдет Rule-based подход (отдельная обработка)

- токенизатор принимает в себя определенные параметры, каждый из которых тоже имеет значимый эффект на результат:
```
- example['context или что передается']
- trancation = True | False (срез до максимальной длины)
- padding = True | False (дополнение до максимальной длинны)
(
    # "КТ" → [101, 12345, 102, 0, 0, 0, ..., 0]  # 1 полезный токен + 127 паддингов
    # Длинный вопрос → [101, 123, 456, ..., 102]  # 128 полезных токенов)

- БЕЗ  trancation,padding = True батчи разной длины  -> НЕЛЬЗЯ ОБУЧАТЬ!!!

- max_length 
- return_tensors="pt или tf" ('Pytorch(cuda() или cpu()) или TensorFLow(cuda()), разные форматы 
представления векторизированных данных в тензорном виде, первый используеться для обучения и тестирования моделей, второй служуит больше форматом отптимизации для выхода чего-то крайне тяжелого в продакшен')
```
```
допишу
```


### Векторизация
```
допишу
```

### Вопрос-ответ (разделяется на классификацию вопрос-ответ, семантический поиск ответов без обучения, последовательную классификация с контекстом)

- разделяется на классификацию вопрос-ответ подразумевает(кроме базовой предобработки):
```
- разметку на хорошие и плохие примеры;
- mapping перед передачей в токенизатор;
- обучение модели (fine-tuning);
- 500+ idшников;
- построение поисковой логики;
- пайплайн для обработки запроса в систему и вывода ответа;
- тесты (постобработку) для валидации модели, написанные зачистую ручками; 
```

- практика показала:
```
- 1-8 токенов (длина idшника)
- intent classification
- sequenceClassification
- 100+-10 id
- точность на узкой выборке 0,5

- 1-15 токенов (длина idшника)
- intent classification
- sequenceClassification
- 1000+-10 id
- точность на узкой выборке 0,8
```

```
допишу
```

### Классификация намерений (анализ тональности)

- с учителем, подразумевает понимание намерений с заранее размеченными категориями;

- {'wanna buy a house':'intent to buy';'been thinking to get rid of some old stuff':'intent to sell'}
- {'really hate today':'bad';'today seems kinda nice':'good'}

- практика показала

```
допишу
```

### Вопросы для разбора
```
- как правильно работать с короткими вопросами наравне с длинными
- насколько разумно гибридное решение для QA, раделение на множество rule-based и несколько размных fine-tuning моделей в одной системе
- self-attention архитектура модели есть, что это и какие есть еще
- как грамотно расширять ML fine-tuning решения
- как именно помагает offset_mapping для при QA последовательной контекстоной классификации, как срезы токенов в словаре idшника помагают при обучении модели(предсказание места нахождения ответа в контексте кажется бредовым)

```