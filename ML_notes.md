# Заметки и выводы, которые я понемногу делаю, изучая разные подходы в ML

### Тут будет разделение на ML classic --- MLM/LLM --- AutoML и все, что с ними связано
```
потом опишу тут полноценное дерево, пока буду писать только конкретные выводы 
```

### Работа через pipeline transformers (few-shot -- zero-shor learning)

-  pipeline подразумевает верхоуровневое решение, надстройку над токенизацией, векторизацией и части предобработкой;

- не адаптирован под все решения на рынке ML;

- редко подходит под реальный продакшен;

### Токенизация 

- https://habr.com/ru/articles/854664/ 

- токенизация представляет собой процесс разделения входных элементов, которыми могут быть к примеру запросы пользователя, элементы словаря, в целом любые предложения и разделения их на отдельные элементы, дробление их, чтобы их потом было прощее аппроксимировать (преобразовывать для упрощение чтения этих данных машиной);

- токенизаторы бывают разные( BERT и GPT токенизаторы отличаются), и имеют в себе заранее заложенный словарь слов и символов, которые и позволяет разделять входные данные на токены;

- конечный вид токенизированного объекта зависит от типа токенизации и словаря самого токенизатора;

- в рабочей программе импортируются определенные токенизаторы под определенную задачу (задачу классификации намерений и классификация вопрос-ответ); 

- к примему если слова на входе нет словаре токенизатора, он может разделить его некоректно, чтобы окажет негативное влияние на результат аппроксимации;  

- НЕ РЕКОМЕНДУЕТСЯ ТОКЕНИЗИРОВАТЬ ИЛИ СКАРМЛИВАТЬ МОДЕЛЯМ:
- для них лучше подойдет Rule-based подход (отдельная обработка)
```
- сущности (фамилии, номера, аббревиатуры, адреса);
- справочные материалы (расписание работы, телефоны, цены, размеры);
- простые паттерны (приветствие, прощание, типизированные для рабочего случая короткие idшники);
- {
    'КТ':'плохой пример';
    'КТ - это часто используемая аабревиатрура для контрольной точки':'хороший пример'
}
```

- токенизатор принимает в себя определенные параметры, каждый из которых тоже имеет значимый эффект на результат:
```
- example['context или что передается']
- trancation
- padding
- max_length
- return_tensors="pt или tf" ('Pytorch(cuda() или cpu()) или TensorFLow(cuda()), разные форматы тензоного представления векторизированных данных в тензорном виде, первый используеться для обучения и тестирования моделей, второй служуит больше форматом отптимизации для выхода чего-то крайне тяжелого в прдакшен')
```
```
допишу
```


### Векторизация
```
допишу
```

### Вопрос-ответ (разделяется на классификацию вопрос-ответ, семантический поиск ответов без обучения, последовательную классификация с контекстом)

- разделяется на классификацию вопрос-ответ подразумевает(кроме базовой предобработки):
```
- разметку на хорошие и плохие примеры;
- mapping перед передачей в токенизатор;
- обучение модели (fine-tuning);
- 500+ idшников;
- построение поисковой логики;
- пайплайн для обработки запроса в систему и вывода ответа;
- тесты (постобработку) для валидации модели, написанные зачистую ручками; 
```
```
допишу
```

### Классификация намерений (анализ тональности)

- с учителем, подразумевает понимание намерений с заранее размеченными категориями;

- {'wanna by a house':'intent to buy';'been thinking to get rid of some old stuff':'intent to sell'}
- {'really hate today':'bad';'today seems kinda nice':'good'}

- практика показала

```
допишу
```

### Вопросы для разбора
```
- как правильно работать с короткими вопросами на ровне с длинными
- насколько разумно гибридное решение для QA, раделение на множество rule-based и несколько размных fine-tuning моделей в одной системе
- self-attention архитектура модели есть, что это и какие есть еще
- как грамотно расширять ML fine-tuning решения
- как именно помагает offset_mapping для при QA последовательной контекстоной классификации, как срезы токенов в словаре idшника помагают при обучении модели(предсказание места нахождения ответа в контексте кажется бредовым)

```