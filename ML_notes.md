# Заметки и выводы, которые я понемногу делаю, изучая разные подходы в ML

- https://habr.com/ru/companies/slurm/articles/768806/ 

### Базовое ML tree
```markdown
### Традиционное машинное обучение
|__Классификация (Classification)
|____Бинарная классификация (Binary)
|____Многоклассовая классификация (Multiclass)
|____Многометочная классификация (Multilabel)
|__Регрессия (Regression)
|____Линейная регрессия (Linear)
|____Логистическая регрессия (Logistic)
|__Алгоритмы
|____Дерево решений (Decision Tree)
|____Случайный лес (Random Forest)
|____k-ближайших соседей (k-NN)
|____Градиентный бустинг (Gradient Boosting)
|______XGBoost
|______LightGBM
|______CatBoost

### Нейронные сети
|__Архитектуры
|____Сверточные сети (CNN)
|____Рекуррентные сети (RNN)
|____Трансформеры (Transformers)
|__Применения
|____Обработка изображений (Computer Vision)
|____Обработка текста (NLP)
|____Обработка звука (Audio)

### Большие языковые модели (LLM)
|__Методы адаптации
|____Точная настройка (Fine-tuning)
|____Обучение с малым числом примеров (Few-shot)
|____Инженерия промптов (Prompt Engineering)
|__Архитектуры
|____GPT (авторегрессионная)
|____BERT (кодировщик)
|____T5 (кодировщик-декодировщик)

### Компьютерное зрение
|__Классификация
|____Распознавание объектов (Object Recognition)
|____Распознавание лиц (Face Recognition)
|__Сегментация
|____Семантическая сегментация (Semantic)
|____Сегментация экземпляров (Instance)
|__Генерация
|____Генеративно-состязательные сети (GANs)
|____Диффузионные модели (Diffusion)

### Обработка естественного языка (NLP)
|__Классификация текста
|____Анализ тональности (Sentiment)
|____Классификация тем (Topic)
|__Генерация текста
|____Машинный перевод (Translation)
|____Суммаризация текста (Summarization)
|____Генерация кода (Code Generation)
|__Извлечение информации
|____Распознавание сущностей (NER)
|____Вопросно-ответные системы (QA)

### Специализированные методы
|__Обучение с подкреплением (RL)
|____Глубокое RL (Deep Reinforcement Learning)
|__Трансферное обучение (Transfer Learning)
|____Доменная адаптация (Domain Adaptation)
|__Мультимодальное обучение (Multimodal)
|____Текст + изображения
|____Аудио + видео

### Оценка моделей
|__Метрики качества
|____Точность, полнота, F-мера (Precision, Recall, F1)
|____ROC-AUC
|____BLEU, ROUGE (для текста)
|__Валидация
|____Кросс-валидация (Cross-validation)
|____Разделение на выборки (Train/Test/Validation)
```

### Работа через pipeline transformers (few-shot -- zero-shot learning)

-  pipeline подразумевает верхоуровневое решение, надстройку над токенизацией, векторизацией и части предобработкой;

- не адаптирован под все решения на рынке ML;

- редко подходит под реальный продакшен;

- низкая точность при zero-shot learning (<0.1).

### Токенизация 

- https://habr.com/ru/articles/854664/ 

- токенизация представляет собой процесс разделения входных элементов, которыми могут быть к примеру запросы пользователя, элементы словаря, в целом любые предложения и разделения их на отдельные элементы, дробление их, чтобы их потом было прощее аппроксимировать (преобразовывать для упрощение чтения этих данных машиной);

- токенизаторы бывают разные( BERT и GPT токенизаторы отличаются), и имеют в себе заранее заложенный словарь слов и символов, которые и позволяет разделять входные данные на токены;

- конечный вид токенизированного объекта зависит от типа токенизации и словаря самого токенизатора;

- в рабочей программе импортируются определенные токенизаторы под определенную задачу (задачи классификации намерений и классификация вопрос-ответ); 

- к примему если слова на входе нет словаре токенизатора, он может разделить его некоректно, чтобы окажет негативное влияние на результат аппроксимации;  

- НЕ РЕКОМЕНДУЕТСЯ ТОКЕНИЗИРОВАТЬ ИЛИ СКАРМЛИВАТЬ МОДЕЛЯМ:
```
- сущности (фамилии, номера, аббревиатуры, адреса);
- справочные материалы (расписание работы, телефоны, цены, размеры);
- простые паттерны (приветствие, прощание, типизированные для рабочего случая короткие idшники);
- {
    'КТ':'плохой пример';
    'КТ - это часто используемая аабревиатрура для контрольной точки':'хороший пример'
}
```
- для них лучше подойдет Rule-based подход (отдельная обработка)

- токенизатор принимает в себя определенные параметры, каждый из которых тоже имеет значимый эффект на результат:
```
- example['context или что передается']
- trancation = True | False (срез до максимальной длины)
- padding = True | False (дополнение до максимальной длинны)
(
    # "КТ" → [101, 12345, 102, 0, 0, 0, ..., 0]  # 1 полезный токен + 127 паддингов
    # Длинный вопрос → [101, 123, 456, ..., 102]  # 128 полезных токенов)

- БЕЗ  trancation,padding = True батчи разной длины  -> НЕЛЬЗЯ ОБУЧАТЬ!!!

- max_length = int (
    '2-3 слова':64;
    '5-10 слов':128;
    '10-20+ слов':'256
)
- return_tensors="pt или tf" ('Pytorch(cuda() или cpu()) или TensorFLow(cuda()), разные форматы 
представления векторизированных данных в тензорном виде, первый используеться для обучения и тестирования моделей, второй служуит больше форматом отптимизации для выхода чего-то крайне тяжелого в продакшен')
```
```
допишу
```


### Векторизация
```
допишу
```

### Вопрос-ответ (разделяется на классификацию вопрос-ответ, семантический поиск ответов без обучения, последовательную классификация с контекстом)

- разделяется на классификацию вопрос-ответ подразумевает(кроме базовой предобработки):
```
- разметку на хорошие и плохие примеры;
- mapping перед передачей в токенизатор;
- обучение модели (fine-tuning);
- 500+ idшников;
- построение поисковой логики;
- пайплайн для обработки запроса в систему и вывода ответа;
- тесты (постобработку) для валидации модели, написанные зачистую ручками; 
```

- практика показала:
```
- 1-8 токенов (длина idшника)
- intent classification
- sequenceClassification
- 100+-10 id
- точность на узкой выборке 0,5

- 1-15 токенов (длина idшника)
- intent classification
- sequenceClassification
- 1000+-10 id
- точность на узкой выборке 0,8
```

```
допишу
```

### Классификация намерений (анализ тональности)

- с учителем, подразумевает понимание намерений с заранее размеченными категориями;

- {'wanna buy a house':'intent to buy';'been thinking to get rid of some old stuff':'intent to sell'}
- {'really hate today':'bad';'today seems kinda nice':'good'}

- практика показала

```
допишу
```

### Вопросы для разбора
```
- как правильно работать с короткими вопросами наравне с длинными
- насколько разумно гибридное решение для QA, раделение на множество rule-based и несколько размных fine-tuning моделей в одной системе
- self-attention архитектура модели есть, что это и какие есть еще
- как грамотно расширять ML fine-tuning решения
- как именно помагает offset_mapping для при QA последовательной контекстоной классификации, как срезы токенов в словаре idшника помогают при обучении модели(предсказание места нахождения ответа в контексте кажется бредовым)
- класстеризация и ей подобные - это classic ML или предобработка, или что-то другое?
```